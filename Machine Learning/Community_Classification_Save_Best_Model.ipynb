{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Save Best Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjiSiwi/arunika-temuin/blob/master/Machine%20Learning/Community_Classification_Save_Best_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47xD24xIdu_-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXBqYwjwdvAH"
      },
      "source": [
        "dataset = pd.read_excel('preprocessed_RIASEC_5Class_V2.xlsx', engine = 'openpyxl')\n",
        "dataset.head(n = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OouGdqxdvAJ"
      },
      "source": [
        "dataset.drop(columns = ['Unnamed: 0'], inplace = True) # Uncomment this line if filename == 'preprocessed_RIASEC_Class3.xlsx'\n",
        "data_cols = list(dataset.columns)\n",
        "data_cols.remove('major')\n",
        "data = dataset[data_cols].values\n",
        "labels = dataset['major'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmECJbXdvAK"
      },
      "source": [
        "labels = labels.reshape((1, labels.shape[0]))\n",
        "transformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(sparse = False), [0])], \n",
        "                                remainder = 'passthrough')\n",
        "labels = transformer.fit_transform(labels.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU46TWRqdvAL"
      },
      "source": [
        "def create_model(input_shape, learning_rate = 1e-4, dropout_rate = 0.2, optimizer = None):\n",
        "    \"\"\"\n",
        "    Create a three layers DNN model with 2 dropout layers for regularization. \n",
        "    \n",
        "    Keyword Argument:\n",
        "    input_shape -- a tuple defining the input_shape of the first layer. The value equals to the number of\n",
        "                   features used.\n",
        "    learning_rate -- defines the learning rate to be used for the default Adam optimizer. Default\n",
        "                     value is 1e-4.\n",
        "    dropout_rate -- defines the percentage of total weights to be dropped. Default value is 0.2, \n",
        "                    meaning 20% of total weights are zeroed.\n",
        "    optimizer -- if None, Adam will be used as default optimizer. Pass a tf.keras.optimizers method\n",
        "                 to use another optimizer.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units = 256, input_shape = input_shape, activation = 'relu',\n",
        "                              kernel_initializer = tf.keras.initializers.GlorotNormal(42)),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(units = 128, activation = 'relu', kernel_initializer = tf.keras.initializers.GlorotNormal(42)),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(units = 5, activation = 'softmax', kernel_initializer = tf.keras.initializers.GlorotNormal(42))\n",
        "    ])\n",
        "    if not optimizer:\n",
        "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                      loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twe6zvX7dvAN"
      },
      "source": [
        "def predict_assess(model, validation_data, validation_label):\n",
        "    \"\"\"\n",
        "    Make predictions based on an array of data and assess the prediction using \n",
        "    accuracy, precision, and recall.\n",
        "    \n",
        "    Keyword Argument:\n",
        "    model -- the model for making predictions.\n",
        "    \n",
        "    validation_data -- an array of data for making predictions.\n",
        "    \n",
        "    validation_label -- an array of labels of the data in validation_data.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(validation_data)\n",
        "    predictions = np.array([np.argmax(prediction) for prediction in predictions])\n",
        "    actuals = np.array([np.argmax(actual) for actual in validation_label])\n",
        "    accuracy = accuracy_score(predictions, actuals)\n",
        "    precision = precision_score(predictions, actuals, average = 'weighted')\n",
        "    recall = recall_score(predictions, actuals, average = 'weighted')\n",
        "    return accuracy, precision, recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-3541zUdvAQ"
      },
      "source": [
        "def training_report(accuracy, precision, recall, num_features = None):\n",
        "    \"\"\"\n",
        "    Print a training report based on model's performance. \n",
        "    \n",
        "    Keyword Argument:\n",
        "    accuracy -- the accuracy score of the model.\n",
        "    \n",
        "    precision -- precision score.\n",
        "    \n",
        "    recall -- recall score.\n",
        "    \n",
        "    num_features -- a scalar value defining the number of features used during training.\n",
        "                    If None, all features are assumed to be in use.\n",
        "    \"\"\"\n",
        "    if not num_features:\n",
        "        print('Accuracy, Precision, and Recall for all features')\n",
        "    else:\n",
        "        print('Accuracy, Precision, and Recall for {} features'.format(num_features))\n",
        "    print('Accuracy: {}'.format(acc))\n",
        "    print('Precision: {}'.format(prec))\n",
        "    print('Recall: {}'.format(rec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dx9e6pFdvAS"
      },
      "source": [
        "training_history = {'num_features': [], 'acc': [], 'prec': [], 'rec': []}\n",
        "def add_to_history(hist_dict, n_features, acc, prec, rec):\n",
        "    \"\"\"\n",
        "    Add model's performance result to hist_dict dictionary containing 'num_features',\n",
        "    'acc', 'prec', and 'rec' keys. The value of all keys are in lists.\n",
        "    \n",
        "    Keyword Argument:\n",
        "    hist_dict -- the dictionary where training performance will be saved. Must contain\n",
        "                 'num_features', 'acc', 'prec', and 'rec' keys in which all of them must be\n",
        "                 lists.\n",
        "    \n",
        "    n_features -- number of features used during training. The value will be added to \n",
        "                  'num_features'.\n",
        "                  \n",
        "    acc -- the obtained accuracy of a model.\n",
        "    \n",
        "    prec -- precision score of a model.\n",
        "    \n",
        "    rec -- recall score.\n",
        "    \"\"\"\n",
        "    hist_dict['num_features'].append(n_features)\n",
        "    hist_dict['acc'].append(acc)\n",
        "    hist_dict['prec'].append(prec)\n",
        "    hist_dict['rec'].append(rec)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0gZTV26dvAU"
      },
      "source": [
        "data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJQpZH8zdvAV"
      },
      "source": [
        "classifier = RandomForestClassifier(n_estimators = 1500, random_state = 42, n_jobs = -1)\n",
        "classifier.fit(data_train, label_train)\n",
        "feat_labels = dataset.columns[:-1]\n",
        "importances = classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYUe9PN2dvAX"
      },
      "source": [
        "plt.figure(figsize = (12, 4))\n",
        "plt.title('Feature Importances')\n",
        "plt.bar(range(data_train.shape[1]), importances[indices], color='lightblue', align='center')\n",
        "plt.xticks(range(data_train.shape[1]), feat_labels[indices], rotation=90)\n",
        "plt.xlim([-1, data_train.shape[1]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2mV7-pbdvAY"
      },
      "source": [
        "n_features = 52\n",
        "cols = feat_labels[indices[:n_features]]\n",
        "new_data = dataset[cols].values\n",
        "data_train, data_test, label_train, label_test = train_test_split(new_data, labels, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mIKQbanwdvAZ"
      },
      "source": [
        "# model = create_model(input_shape = (n_features, ), learning_rate = 1e-5)\n",
        "# history = model.fit(data_train, label_train, batch_size = 16, epochs = 80)\n",
        "# acc, prec, rec = predict_assess(model, data_test, label_test)\n",
        "# training_report(acc, prec, rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-guqyLudvAb"
      },
      "source": [
        "k_fold = KFold(n_splits = 10)\n",
        "for train_index, test_index in k_fold.split(data):\n",
        "    data_train, data_test = new_data[train_index], new_data[test_index]\n",
        "    label_train, label_test = labels[train_index], labels[test_index]\n",
        "    model = create_model(input_shape = (n_features, ), dropout_rate = 0.5)\n",
        "    history = model.fit(data_train, label_train, batch_size = 16, epochs = 80)\n",
        "    acc, prec, rec = predict_assess(model, data_test, label_test)\n",
        "    training_report(acc, prec, rec)\n",
        "    try:\n",
        "        if acc > training_history['acc'][-1] or not training_history['acc']:\n",
        "            model.save('models/best_model.h5')\n",
        "    except:\n",
        "        pass\n",
        "    add_to_history(training_history, n_features, acc, prec, rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CrKqy1WdvAe"
      },
      "source": [
        "training_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtWXEsYNdvAf"
      },
      "source": [
        "# history_df = pd.DataFrame(training_history)\n",
        "# history_df.to_csv('10Fold_{}.csv'.format(n_features))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}