{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Tuning Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjiSiwi/arunika-temuin/blob/master/Machine%20Learning/Community_Classification_Tuning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju46fJHy700i"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hur01Ire700m"
      },
      "source": [
        "import tensorflow.python.util.deprecation as deprecation\n",
        "\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzyc_J4p700n"
      },
      "source": [
        "dataset = pd.read_excel('preprocessed_RIASEC_5Class_V2.xlsx', engine = 'openpyxl')\n",
        "dataset.head(n = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjcOOHQg700o"
      },
      "source": [
        "dataset.drop(columns = ['Unnamed: 0'], inplace = True)\n",
        "data_cols = list(dataset.columns)\n",
        "data_cols.remove('major')\n",
        "data = dataset[data_cols].values\n",
        "labels = dataset['major'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxPPc27g700p"
      },
      "source": [
        "labels = labels.reshape((1, labels.shape[0]))\n",
        "transformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(sparse = False), [0])], \n",
        "                                remainder = 'passthrough')\n",
        "labels = transformer.fit_transform(labels.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQjYvyUh700q"
      },
      "source": [
        "def create_model(input_shape, learning_rate = 1e-4, dropout_rate = 0.2, optimizer = None, \n",
        "                 kernel_init = None):\n",
        "    \"\"\"\n",
        "    Create a three layers DNN model with 2 dropout layers for regularization. \n",
        "    \n",
        "    Keyword Argument:\n",
        "    input_shape -- a tuple defining the input_shape of the first layer. The value equals to the number of\n",
        "                   features used.\n",
        "    learning_rate -- defines the learning rate to be used for the default Adam optimizer. Default\n",
        "                     value is 1e-4.\n",
        "    dropout_rate -- defines the percentage of total weights to be dropped. Default value is 0.2, \n",
        "                    meaning 20% of total weights are zeroed.\n",
        "    optimizer -- if None, Adam will be used as default optimizer. Pass a tf.keras.optimizers method\n",
        "                 to use another optimizer.\n",
        "    \"\"\"\n",
        "    if not kernel_init:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(units = 128, input_shape = input_shape, activation = 'relu'),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(units = 64, activation = 'relu'),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(units = 5, activation = 'softmax')\n",
        "        ])\n",
        "    else:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(units = 128, input_shape = input_shape, kernel_initializer = kernel_init,\n",
        "                                  activation = 'relu'),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(units = 64, kernel_initializer = kernel_init, activation = 'relu'),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(units = 5, kernel_initializer = kernel_init, activation = 'softmax')\n",
        "        ])\n",
        "    if not optimizer:\n",
        "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                      loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7IlvVlz700r"
      },
      "source": [
        "def predict_assess(model, validation_data, validation_label):\n",
        "    \"\"\"\n",
        "    Make predictions based on an array of data and assess the prediction using \n",
        "    accuracy, precision, and recall.\n",
        "    \n",
        "    Keyword Argument:\n",
        "    model -- the model for making predictions.\n",
        "    \n",
        "    validation_data -- an array of data for making predictions.\n",
        "    \n",
        "    validation_label -- an array of labels of the data in validation_data.\n",
        "    \"\"\"\n",
        "    predictions = model.predict(validation_data)\n",
        "    predictions = np.array([np.argmax(prediction) for prediction in predictions])\n",
        "    actuals = np.array([np.argmax(actual) for actual in validation_label])\n",
        "    accuracy = accuracy_score(predictions, actuals)\n",
        "    precision = precision_score(predictions, actuals, average = 'weighted')\n",
        "    recall = recall_score(predictions, actuals, average = 'weighted')\n",
        "    return accuracy, precision, recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQhgNOrw700s"
      },
      "source": [
        "def training_report(accuracy, precision, recall, num_features = None):\n",
        "    \"\"\"\n",
        "    Print a training report based on model's performance. \n",
        "    \n",
        "    Keyword Argument:\n",
        "    accuracy -- the accuracy score of the model.\n",
        "    \n",
        "    precision -- precision score.\n",
        "    \n",
        "    recall -- recall score.\n",
        "    \n",
        "    num_features -- a scalar value defining the number of features used during training.\n",
        "                    If None, all features are assumed to be in use.\n",
        "    \"\"\"\n",
        "    if not num_features:\n",
        "        print('Accuracy, Precision, and Recall for all features')\n",
        "    else:\n",
        "        print('Accuracy, Precision, and Recall for {} features'.format(num_features))\n",
        "    print('Accuracy: {}'.format(acc))\n",
        "    print('Precision: {}'.format(prec))\n",
        "    print('Recall: {}'.format(rec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nV6B_SS700t"
      },
      "source": [
        "training_history = {'num_features': [], 'acc': [], 'prec': [], 'rec': []}\n",
        "def add_to_history(hist_dict, n_features, acc, prec, rec):\n",
        "    \"\"\"\n",
        "    Add model's performance result to hist_dict dictionary containing 'num_features',\n",
        "    'acc', 'prec', and 'rec' keys. The value of all keys are in lists.\n",
        "    \n",
        "    Keyword Argument:\n",
        "    hist_dict -- the dictionary where training performance will be saved. Must contain\n",
        "                 'num_features', 'acc', 'prec', and 'rec' keys in which all of them must be\n",
        "                 lists.\n",
        "    \n",
        "    n_features -- number of features used during training. The value will be added to \n",
        "                  'num_features'.\n",
        "                  \n",
        "    acc -- the obtained accuracy of a model.\n",
        "    \n",
        "    prec -- precision score of a model.\n",
        "    \n",
        "    rec -- recall score.\n",
        "    \"\"\"\n",
        "    hist_dict['num_features'].append(n_features)\n",
        "    hist_dict['acc'].append(acc)\n",
        "    hist_dict['prec'].append(prec)\n",
        "    hist_dict['rec'].append(rec)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5_pejF700u"
      },
      "source": [
        "data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IEe7lPz700u"
      },
      "source": [
        "classifier = RandomForestClassifier(n_estimators = 1500, random_state = 42, n_jobs = -1)\n",
        "classifier.fit(data_train, label_train)\n",
        "feat_labels = dataset.columns[:-1]\n",
        "importances = classifier.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu3975-4700v"
      },
      "source": [
        "plt.figure(figsize = (12, 4))\n",
        "plt.title('Feature Importances')\n",
        "plt.bar(range(data_train.shape[1]), importances[indices], color='lightblue', align='center')\n",
        "plt.xticks(range(data_train.shape[1]), feat_labels[indices], rotation=90)\n",
        "plt.xlim([-1, data_train.shape[1]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhzUtWRK700v"
      },
      "source": [
        "n_features = 52\n",
        "new_data = dataset[feat_labels[indices[:n_features]]].values\n",
        "data_train, data_test, label_train, label_test = train_test_split(new_data, labels, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdfQTU_700w"
      },
      "source": [
        "save_model_path = os.path.join(os.getcwd(), 'models')\n",
        "if not os.path.isdir(save_model_path):\n",
        "    os.mkdir(save_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OjtSUFb4700w"
      },
      "source": [
        "train_acc = []\n",
        "combinations = []\n",
        "initializers = [tf.keras.initializers.GlorotUniform(), tf.keras.initializers.GlorotNormal()]\n",
        "dropout_rates = [value / 10 for value in range(1, 6)]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = save_model_path + '\\rec_model.h5', save_best_only = True,\n",
        "                                                      monitor = 'val_accuracy', verbose = 1)\n",
        "for initializer in initializers:\n",
        "    for rate in dropout_rates:\n",
        "        for size in batch_sizes:\n",
        "            combinations.append((initializers.index(initializer) + 1, rate, size))\n",
        "            model = create_model(input_shape = (n_features, ), dropout_rate = rate, kernel_init = initializer)\n",
        "            history = model.fit(data_train, label_train, batch_size = size, epochs = 80, \n",
        "                                validation_data = (data_test, label_test), callbacks = [model_checkpoint])\n",
        "            acc, prec, rec = predict_assess(model, data_test, label_test)\n",
        "            training_report(acc, prec, rec)\n",
        "            train_acc.append(sum(history.history['accuracy']) / len(history.history['accuracy']))\n",
        "            add_to_history(training_history, n_features, acc, prec, rec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVbd82IF700w"
      },
      "source": [
        "for key in training_history.keys():\n",
        "    if len(training_history[key]) != 40:\n",
        "        training_history[key] = training_history[key][-40:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4xv0YtL700x"
      },
      "source": [
        "training_history['combinations'] = combinations\n",
        "training_history['train_acc'] = train_acc\n",
        "try:\n",
        "    del training_history['num_features']\n",
        "except:\n",
        "    pass\n",
        "history_df = pd.DataFrame(training_history)\n",
        "history_df.to_csv('history_lesser_dense.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX1pOjlw700x"
      },
      "source": [
        "# history_df = pd.DataFrame(training_history)\n",
        "# history_df.to_csv('history.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}